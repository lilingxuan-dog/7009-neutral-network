{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Project1 Training of a Fully-Connected Neural Network\n",
    "### MNIST Dataset - Autonomous Learning of Neural Networks\n",
    "*——————  Neural network learning and reasoning based on numpy*"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.  Visualization of MNIST dataset\n",
    "First, download mnist.py from website and locate it in your working directory. You can then prepare the MNIST data in Python as follows. There is not much introduction to mnist.py here\n",
    "\n",
    "The mnist dataset mainly includes four files:\n",
    "- x_train : 60,000x784 numpy array that each row contains flattened version of training images.\n",
    "- t_train : 1x60,000 numpy array that each component is true label of the corresponding training images.\n",
    "- x_test : 10,000x784 numpy array that each row contains flattened version of test images.\n",
    "- t_test : 1x10,000 numpy array that each component is true label of the corresponding test images."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# mnist dataset image display\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "from PIL import Image\n",
    "def img_show(img):\n",
    "    pil_img = Image.fromarray(np.uint8(img))\n",
    "    pil_img.show()\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=True, normalize=False)\n",
    "img = x_train[5]\n",
    "lable = t_train[5]\n",
    "print(lable)\n",
    "\n",
    "print(img.shape)\n",
    "img = img.reshape(28, 28)   # Change the shape of the image to its original shape\n",
    "print(img.shape)\n",
    "\n",
    "img_show(img)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![](图片2.png)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.Handwritten digit recognition based on numpy"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the process of neural network implementation, a 3-layer neural network is used to realize the processing from input to output"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img alt=\"三层神经网络\" height=\"300\" src=\"https://img-blog.csdnimg.cn/b8c6d8838a2e48b285a1bedada0eeeb0.png\" width=\"600\"/>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the inference processing of the neural network implemented on the MNIST dataset, the input layer of the neural network has 784 neurons and the output layer has 10 neurons. The number 784 in the input layer comes from the image size of 28*28=784, and the number 10 in the output layer comes from the 10-category classification. In addition, this neural network has two hidden layers, the first hidden layer is set to have 50 neurons, and the second hidden layer is set to have 100 neurons"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This set of data is the shape of the weights of each layer of the neural network\n",
    "![](图片1.png)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The accuracy rate of this test is finally about 94.5%.\n",
    "![准确率](图片3.png)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This is the data image display for this test\n",
    "![图像 ](图片4.png)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "# matplotlib_ data visualization tool for drawing two-dimensional and three-dimensional charts;\n",
    "# pyplot_ for the current graph\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "#The sigmoid function is used as the activation function,\n",
    "#and the function of the activation function is to decide how to activate the sum of the input signals\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_grad(x):\n",
    "    return (1.0 - sigmoid(x)) * sigmoid(x)\n",
    "\n",
    "def softmax(x):\n",
    "    if x.ndim == 2:\n",
    "        x = x.T\n",
    "        x = x - np.max(x, axis=0)\n",
    "        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "        return y.T\n",
    "\n",
    "    x = x - np.max(x)  # softmax - Spill Countermeasures\n",
    "    return np.exp(x) / np.sum(np.exp(x))\n",
    "\n",
    "#Implementation of min-batch version cross entropy error\n",
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "\n",
    " # When the supervised data is one-hot-vector, it is converted to the index of the correct solution label\n",
    "    if t.size == y.size:\n",
    "        t = t.argmax(axis=1)\n",
    "\n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size\n",
    "#np.arange([start, ]stop, [step, ]dtype=None)\n",
    "\n",
    "def numerical_gradient(f, x):\n",
    "    h = 1e-4  # 0.0001\n",
    "    grad = np.zeros_like(x)\n",
    "\n",
    "    for idx in range(x.size):\n",
    "        tmp_val = x[idx]\n",
    "        x[idx] = tmp_val + h\n",
    "        fxh1 = f(x)  # f(x+h)\n",
    "\n",
    "        x[idx] = tmp_val - h\n",
    "        fxh2 = f(x)  # f(x-h)\n",
    "        grad[idx] = (fxh1 - fxh2) / (2 * h)\n",
    "        x[idx] = tmp_val  # 还原值\n",
    "\n",
    "    return grad\n",
    "#Read in the MNIST dataset\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "class ThreeLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, mid_size, output_size, weight_init_std=0.01):\n",
    "        print(\"Build Net\")\n",
    "        # Initialize weight and bias parameters\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, mid_size)\n",
    "        self.params['b2'] = np.zeros(mid_size)\n",
    "        self.params['W3'] = weight_init_std * np.random.randn(mid_size, output_size)\n",
    "        self.params['b3'] = np.zeros(output_size)\n",
    "\n",
    "    def predict(self, x):\n",
    "        W1, W2, W3 = self.params['W1'], self.params['W2'], self.params['W3']\n",
    "        b1, b2, b3 = self.params['b1'], self.params['b2'], self.params['b3']\n",
    "\n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        z2 = sigmoid(a2)\n",
    "        a3 = np.dot(z2, W3) + b3\n",
    "        y = softmax(a3)\n",
    "\n",
    "        return y\n",
    "\n",
    "    # Loss function - x: input data, t: supervised data\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return cross_entropy_error(y, t)\n",
    "\n",
    "\n",
    "    # Numerical differentiation\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        grads['W3'] = numerical_gradient(loss_W, self.params['W3'])\n",
    "        grads['b3'] = numerical_gradient(loss_W, self.params['b3'])\n",
    "        return grads\n",
    "\n",
    "\n",
    "    # Error backpropagation\n",
    "    def gradient(self, x, t):\n",
    "        W1, W2, W3 = self.params['W1'], self.params['W2'], self.params['W3']\n",
    "        b1, b2, b3 = self.params['b1'], self.params['b2'], self.params['b3']\n",
    "        grads = {}\n",
    "        print('W1:', W1.shape)\n",
    "        print('W2:', W2.shape)\n",
    "        print('W3:', W3.shape)\n",
    "\n",
    "        batch_num = x.shape[0]\n",
    "\n",
    "        # Forward\n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        z2 = sigmoid(a2)\n",
    "        a3 = np.dot(z2, W3) + b3\n",
    "        y = softmax(a3)\n",
    "\n",
    "\n",
    "        # Backward\n",
    "        dy = (y - t) / batch_num\n",
    "        grads['W3'] = np.dot(z2.T, dy)\n",
    "        grads['b3'] = np.sum(dy, axis=0)\n",
    "\n",
    "        da2 = np.dot(dy, W3.T)\n",
    "        dz2 = sigmoid_grad(a2) * da2\n",
    "        grads['W2'] = np.dot(z1.T, dz2)\n",
    "        grads['b2'] = np.sum(dz2, axis=0)\n",
    "\n",
    "        da1 = np.dot(dz2, W2.T)\n",
    "        dz1 = sigmoid_grad(a1) * da1\n",
    "        grads['W1'] = np.dot(x.T, dz1)\n",
    "        grads['b1'] = np.sum(dz1, axis=0)\n",
    "\n",
    "        return grads\n",
    "\n",
    "    # Accuracy\n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    (x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "    network = ThreeLayerNet(input_size=784, hidden_size=50, mid_size=100, output_size=10)\n",
    "\n",
    "    # Training and validation\n",
    "    batch_size = 150\n",
    "    iter_nums = 15000\n",
    "    train_size = x_train.shape[0]\n",
    "    learning_rate = 0.1\n",
    "    train_loss_list = []\n",
    "    train_acc_list = []\n",
    "    test_acc_list = []\n",
    "\n",
    "    # Average number of repetitions per epoch\n",
    "    iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "    for i in range(iter_nums):\n",
    "        batch_mask = np.random.choice(train_size, batch_size)\n",
    "        x_batch = x_train[batch_mask]\n",
    "        t_batch = t_train[batch_mask]\n",
    "\n",
    "        # Gradient by Error Backpropagation\n",
    "        grad = network.gradient(x_batch, t_batch)\n",
    "\n",
    "        # Update parameters - weight W and weight b\n",
    "        for key in ['W1', 'b1', 'W2', 'b2', 'W3', 'b3']:\n",
    "            network.params[key] -= learning_rate * grad[key]\n",
    "\n",
    "        # Record the learning process\n",
    "        loss = network.loss(x_batch, t_batch)\n",
    "        print('训练次数:' + str(i) + '    loss:' + str(loss))\n",
    "        train_loss_list.append(loss)\n",
    "\n",
    "        # Calculate the recognition accuracy for each epoch\n",
    "        if i % iter_per_epoch == 0:\n",
    "            train_acc = network.accuracy(x_train, t_train)\n",
    "            test_acc = network.accuracy(x_test, t_test)\n",
    "            train_acc_list.append(train_acc)\n",
    "            test_acc_list.append(test_acc)\n",
    "            print(train_acc, test_acc)\n",
    "\n",
    "    print(train_acc_list)\n",
    "    print(test_acc_list)\n",
    "\n",
    "    # Draw Accuracy Graphs\n",
    "    markers = {'train': 'o', 'test': 's'}\n",
    "    x = np.arange(len(train_acc_list))\n",
    "    plt.plot(x, train_acc_list, label='train acc')\n",
    "    plt.plot(x, test_acc_list, label='test acc', linestyle='--')\n",
    "    plt.xlabel(\"epochs\")\n",
    "    plt.ylabel(\"accuracy\")\n",
    "    plt.ylim(0, 1.0)\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
